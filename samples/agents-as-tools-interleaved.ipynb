{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents as Tools with Strands Agents SDK and Claude 4 Interleaved Thinking\n",
    "\n",
    "This notebook demonstrates how to use Strands Agents SDK with Claude 4's **interleaved thinking** capability to orchestrate intelligent workflows with specialist agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Interleaved Thinking\n",
    "\n",
    "### What is Interleaved Thinking?\n",
    "\n",
    "Interleaved thinking is a new capability in Claude 4 models that allows the model to:\n",
    "\n",
    "1. **Think between tool calls**: Process and reason about results before deciding next steps\n",
    "2. **Chain multiple tools with reasoning**: Make sophisticated multi-step decisions\n",
    "3. **Adapt strategies dynamically**: Change approach based on intermediate results\n",
    "\n",
    "### How It Works\n",
    "\n",
    "There are a lot of similarities between Agent's event loop implemented with and without interleaved thinking:\n",
    "```\n",
    "Query ‚Üí LLM is thinking -> LLM decides to call a Tool -> Event Loop calls the Tool -> Ouput is sent back to LLM -> [ this continues until LLM no longer needs to call any tools - it rendered the Final Answer ]\n",
    "```\n",
    "\n",
    "The main difference you'll notice with the interleaved thinking is that Event loop is acting on LLM's \"thoughts\", rather than \"decisions\". Notice the second link in the loop above, called \"thinking\". In a traditional event loop, the thoughts are hidden. We have to wait until LLM renders either a decision to call a tool or produces the Final Answer. \n",
    "\n",
    "In case of interleaved thinking, LLM is \"leaking\" its thoughts into the even loop while it's still in that second step - \"LLM is thinking\" - and event loop is configured to executed the tools as soon as LLM \"thinks\" about doing it. What this means is that by the time LLM is done thinking, it actually has the Final Answer, on the very first \"decision\". \n",
    "\n",
    "\n",
    "### Enabling Interleaved Thinking\n",
    "\n",
    "To enable this feature with Strands and Bedrock:\n",
    "- Set `temperature=1` (required when thinking is enabled)\n",
    "- Add beta header: `\"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"]`\n",
    "- Configure reasoning budget: `\"reasoning_config\": {\"type\": \"enabled\", \"budget_tokens\": 3000}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install strands-agents strands-agents-tools python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.models import bedrock\n",
    "\n",
    "# bedrock.DEFAULT_BEDROCK_MODEL_ID = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "bedrock.DEFAULT_BEDROCK_MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Specialist Agents as Tools\n",
    "\n",
    "First, we are going to create four specialist agents using the Strands `@tool` decorator:\n",
    "- **Researcher**: Gathers factual information\n",
    "- **Data Analyst**: Processes and analyzes information\n",
    "- **Fact Checker**: Verifies information accuracy\n",
    "- **Report Writer**: Creates polished final documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import CODE_ANALYSIS_PROMPT\n",
    "from strands.tools.mcp import MCPClient\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import file_read\n",
    "\n",
    "#  Specialist agents implemented as tools using Strands @tool decorator\n",
    "@tool\n",
    "def code_reader(repo_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Code reader specialist that can search through a repository and find relevant content.\n",
    "    \n",
    "    Args:\n",
    "        repo_path: The repository path of the repository to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Reading findings\n",
    "    \"\"\"\n",
    "\n",
    "    bedrock_model = BedrockModel(model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "\n",
    "    result = str()\n",
    "\n",
    "    try:\n",
    "        env = {\"FASTMCP_LOG_LEVEL\": \"DEBUG\",\n",
    "        \"AWS_PROFILE\": \"eldimi-Admin\",\n",
    "        \"AWS_REGION\": \"us-east-1\",\n",
    "                    }\n",
    "\n",
    "        git_repo_mcp_server = MCPClient(\n",
    "            lambda: stdio_client(\n",
    "                StdioServerParameters(\n",
    "                    command=\"uvx\",\n",
    "                    args=[\"awslabs.git-repo-research-mcp-server@latest\"],\n",
    "                    env=env,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with git_repo_mcp_server:\n",
    "\n",
    "            tools = [git_repo_mcp_server.list_tools_sync()]\n",
    "            code_reader_agent = Agent(\n",
    "                model=bedrock_model,\n",
    "                system_prompt=CODE_ANALYSIS_PROMPT,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            result = str(code_reader_agent(repo_path))\n",
    "            print(\"\\n\\n\")\n",
    "    except Exception as e:\n",
    "        return f\"Error processing your query: {str(e)}\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def code_converter(data: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts PostreSQL analytics queries to ClickHouse analytics queries.\n",
    "    \n",
    "    Args:\n",
    "        data: the file paths, code and code description\n",
    "        \n",
    "    Returns:\n",
    "        The converted queries\n",
    "    \"\"\"\n",
    "    # Analyst agent focuses on extracting insights\n",
    "    code_converter_agent = Agent(\n",
    "        model=\"\",\n",
    "        system_prompt=\"\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # Analyze the provided data\n",
    "    result = code_converter_agent(data)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def code_writer(repo_path: str, coverted_code:str) -> str:\n",
    "    \"\"\"\n",
    "    Writes new code in the repository given the provided coverted_code queries\n",
    "    \n",
    "    Args:\n",
    "        repo_path: The path of the repository to write the code to\n",
    "        coverted_code: the converted queries\n",
    "        \n",
    "    Returns:\n",
    "        Fact-check results with accuracy assessment\n",
    "    \"\"\"\n",
    "    code_writer_agent = Agent(\n",
    "        model=\"\",\n",
    "        system_prompt=\"\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # Verify the information\n",
    "    result = code_writer_agent(f\"Fact-check this information: {information}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 4 Orchestrator with Interleaved Thinking\n",
    "\n",
    "Now we create the orchestrator - a Claude 4 agent that uses interleaved thinking to intelligently coordinate the specialist agents.\n",
    "\n",
    "### How the Orchestrator Works:\n",
    "\n",
    "1. Receives a high-level task from the user\n",
    "2. **Thinks** about what information is needed\n",
    "3. Calls the researcher tool to gather initial data\n",
    "4. **Thinks** about the research results and what analysis is needed\n",
    "5. Calls the data analyst to process findings\n",
    "6. **Thinks** about accuracy and verification needs\n",
    "7. May call the fact checker if needed\n",
    "8. **Thinks** about how to present the findings\n",
    "9. Calls the report writer for final output\n",
    "10. **Reflects** on the complete workflow before responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4 Orchestrator with Interleaved Thinking using Strands\n",
    "class StrandsInterlevedWorkflowOrchestrator:\n",
    "    def __init__(self):\n",
    "        # Define the orchestrator system prompt for intelligent workflow coordination\n",
    "        self.system_prompt = \"\"\"You are an intelligent workflow orchestrator with access to specialist agents.\n",
    "\n",
    "        Your role is to intelligently coordinate a workflow using these specialist agents:\n",
    "        - code_reader: Reads a repository content from the file system and searches for ALL postgreSQL analytics queries\n",
    "        - code_converter: Converts the found postgres analytics queries to ClickHouse analytics queries\n",
    "        - code_writer: Replaces the postgres analytics queries implementation with the new ClickHouse analytics queries  \n",
    "\n",
    "        The agents will run sequentially code_reader -> code_converter -> code_writer. \n",
    "        The code_reader might need to run multiple times to identify ALL postgreSQL queries.\n",
    "        The coordinator can re-try and validate accordingly.\n",
    "        The coordinator understands the output of each agent and provides the output if needed as input to the next agent.\n",
    "        \"\"\"\n",
    "    \n",
    "    def run_workflow(self, task: str, enable_interleaved_thinking: bool = True) -> str:\n",
    "        \"\"\"Execute an intelligent workflow for the given task.\n",
    "        \n",
    "        Args:\n",
    "            task: The task to complete\n",
    "            enable_interleaved_thinking: Whether to enable interleaved thinking (default: True)\n",
    "        \n",
    "        The orchestrator will:\n",
    "        1. Understand the task requirements\n",
    "        2. Think about the best approach\n",
    "        3. Coordinate specialist agents\n",
    "        4. Reflect on results between steps\n",
    "        5. Produce a comprehensive output\n",
    "        \"\"\"\n",
    "        thinking_mode = \"WITH interleaved thinking\" if enable_interleaved_thinking else \"WITHOUT interleaved thinking\"\n",
    "        print(f\"\\nStarting intelligent workflow {thinking_mode} for: {task}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Configure Claude 4 with or without interleaved thinking via Bedrock\n",
    "        if enable_interleaved_thinking:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1,  # Required to be 1 when thinking is enabled\n",
    "                additional_request_fields={\n",
    "                    # Enable interleaved thinking beta feature\n",
    "                    \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "                    # Configure reasoning parameters\n",
    "                    \"reasoning_config\": {\n",
    "                        \"type\": \"enabled\",  # Turn on thinking\n",
    "                        \"budget_tokens\": 3000  # Thinking token budget\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1\n",
    "            )\n",
    "        \n",
    "        # Create the orchestrator agent with Claude 4 and specialist tools\n",
    "        orchestrator = Agent(\n",
    "            model=claude4_model,\n",
    "            system_prompt=self.system_prompt,\n",
    "            tools=[code_reader, code_converter, code_writer]\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"Complete this task using intelligent workflow coordination: {task}\n",
    "\n",
    "        Instructions:\n",
    "        1. Think carefully about what information you need to accomplish this task\n",
    "        2. Use the specialist agents strategically - each has unique strengths\n",
    "        3. After each tool use, reflect on the results and adapt your approach\n",
    "        4. Coordinate multiple agents as needed for comprehensive results\n",
    "        5. Ensure accuracy by fact-checking when appropriate\n",
    "        6. Provide a comprehensive final response that addresses all aspects\n",
    "        \n",
    "        Remember: Your thinking between tool calls helps you make better decisions.\n",
    "        Use it to plan, evaluate results, and adjust your strategy.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = orchestrator(prompt)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Workflow failed: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Demo\n",
    "\n",
    "Let's see the orchestrator in action! Watch how it thinking and making tool calling while it's thinking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strands Agents SDK: Claude 4 Interleaved Thinking Workflow Demo\n",
      "======================================================================\n",
      "‚úÖ Orchestrator initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the orchestrator\n",
    "print(\"Strands Agents SDK: Claude 4 Interleaved Thinking Workflow Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    orchestrator = StrandsInterlevedWorkflowOrchestrator()\n",
    "    print(\"‚úÖ Orchestrator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize orchestrator: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Repository path: /Users/eldimi/Documents/ch/HouseClick\n",
      "\n",
      "\n",
      "Starting intelligent workflow WITH interleaved thinking for: /Users/eldimi/Documents/ch/HouseClick\n",
      "======================================================================\n",
      "I need to complete a task that involves analyzing a repository at `/Users/eldimi/Documents/ch/HouseClick` and converting PostgreSQL analytics queries to ClickHouse analytics queries. Based on the available tools and workflow description, I need to:\n",
      "\n",
      "1. Use code_reader to find all PostgreSQL analytics queries in the repository\n",
      "2. Use code_converter to convert those queries to ClickHouse format\n",
      "3. Use code_writer to replace the original queries with the converted ones\n",
      "\n",
      "Let me start by using the code_reader to analyze the repository and find PostgreSQL queries.I'll help you convert PostgreSQL analytics queries to ClickHouse queries in your repository. Let me start by analyzing the repository structure and identifying all PostgreSQL queries.\n",
      "Tool #1: code_reader\n",
      "I'll help you identify ALL PostgreSQL analytics queries within the repository at `/Users/eldimi/Documents/ch/HouseClick`. Let me start by exploring the repository structure to understand what we're working with.\n",
      "Tool #1: access_file\n",
      "Now let me explore the main directories to understand the structure better. Let me start with the `app`, `schemas`, `scripts`, and `data` directories as these are most likely to contain SQL queries.\n",
      "Tool #2: access_file\n",
      "\n",
      "Tool #3: access_file\n",
      "\n",
      "Tool #4: access_file\n",
      "\n",
      "Tool #5: access_file\n",
      "Let me now explore the SQL directory in scripts and check the schema files for PostgreSQL queries:\n",
      "Tool #6: access_file\n",
      "Now let me examine the PostgreSQL schema file and the SQL files I've found:\n",
      "Tool #7: access_file\n",
      "\n",
      "Tool #8: access_file\n",
      "Now let me explore the app directory structure to look for any TypeScript/JavaScript files that might contain embedded SQL queries:\n",
      "Tool #9: access_file\n",
      "\n",
      "Tool #10: access_file\n",
      "This looks promising! Let me check the analytics-related files in both the app and lib directories:\n",
      "Tool #11: access_file\n",
      "Let me also check the other analytics-related files:\n",
      "Tool #12: access_file\n",
      "\n",
      "Tool #13: access_file\n",
      "Now let me check the app/analytics directories to see if there are more queries:\n",
      "Tool #14: access_file\n",
      "\n",
      "Tool #15: access_file\n",
      "Let me check if there are any other directories or files that might contain SQL queries. Let me also examine the Python scripts to see if they contain any SQL:\n",
      "Tool #16: access_file\n",
      "\n",
      "Tool #17: access_file\n",
      "Now let me check the archive directory to see if there are any additional SQL files:\n",
      "Tool #18: access_file\n",
      "\n",
      "Tool #19: access_file\n",
      "\n",
      "Tool #20: access_file\n",
      "\n",
      "Tool #21: access_file\n",
      "Let me check the ClickHouse schema file as well to make sure I haven't missed any PostgreSQL specific queries:\n",
      "Tool #22: access_file\n",
      "Now let me check if there are any other TypeScript or JavaScript files in the app directory that might contain SQL:\n",
      "Tool #23: access_file\n",
      "\n",
      "Tool #24: access_file\n",
      "Let me also check the API directory:\n",
      "Tool #25: access_file\n",
      "\n",
      "Tool #26: access_file\n",
      "Let me check the README file to understand better what this repository is about:\n",
      "Tool #27: access_file\n",
      "Based on my comprehensive search through the HouseClick repository, I have identified ALL PostgreSQL analytics queries. Let me provide a detailed summary of my findings:\n",
      "\n",
      "## Search Query Execution Summary\n",
      "\n",
      "**Search Strategy**: I executed the following search approach to find ALL PostgreSQL queries:\n",
      "1. Explored the complete repository structure recursively\n",
      "2. Examined all SQL files (.sql extensions)\n",
      "3. Scanned all TypeScript/JavaScript files for embedded SQL strings\n",
      "4. Checked Python scripts for SQL queries\n",
      "5. Investigated both current and archived files\n",
      "6. Focused specifically on PostgreSQL queries (excluded ClickHouse queries)\n",
      "\n",
      "**Why I stopped my search**: I concluded my search because I had systematically examined:\n",
      "- All `.sql` schema files\n",
      "- All TypeScript/JavaScript files in the lib and app directories\n",
      "- All Python scripts in the scripts directory\n",
      "- Both current and archived directories\n",
      "- No additional files contained PostgreSQL-specific queries beyond what I found\n",
      "\n",
      "---\n",
      "\n",
      "# PostgreSQL Analytics Queries Found in HouseClick Repository\n",
      "\n",
      "## Summary\n",
      "I discovered **17 unique PostgreSQL analytics queries** within the HouseClick repository. All queries are contained in a single file and are specifically designed for house price analytics, performing aggregations, statistical calculations, and time-series analysis on UK property market data.\n",
      "\n",
      "## Table of Contents\n",
      "- [Main Analytics Query File](#main-analytics-query-file)\n",
      "\n",
      "---\n",
      "\n",
      "## Main Analytics Query File\n",
      "\n",
      "### File: `/app/lib/analytics_queries.ts`\n",
      "\n",
      "**What this file analyzes**: This file contains comprehensive PostgreSQL analytics queries for UK house price data analysis, including price trends over time, property type distributions, ownership duration analysis, statistical calculations, and geographic popularity rankings.\n",
      "\n",
      "---\n",
      "\n",
      "#### Query 1: `soldOverTime`\n",
      "```sql\n",
      "SELECT\n",
      "    to_char(DATE_TRUNC('year', date), 'YYYY-MM-DD') AS year,\n",
      "    COUNT(*) FILTER (WHERE ${condition})::INT AS filtered_count,\n",
      "    ROUND(COUNT(*)::NUMERIC / COUNT(DISTINCT ${column}))::INT AS count\n",
      "FROM uk_price_paid\n",
      "GROUP BY year\n",
      "ORDER BY year ASC;\n",
      "```\n",
      "\n",
      "#### Query 2: `soldOverTimeNoFilter`\n",
      "```sql\n",
      "SELECT\n",
      "    to_char(DATE_TRUNC('year', date), 'YYYY-MM-DD') AS year,\n",
      "    ROUND(COUNT(*)::NUMERIC / COUNT(DISTINCT town))::INT AS count\n",
      "FROM uk_price_paid\n",
      "GROUP BY year\n",
      "ORDER BY year ASC;\n",
      "```\n",
      "\n",
      "#### Query 3: `priceOverTime`\n",
      "```sql\n",
      "SELECT\n",
      "    to_char(date_trunc('month', date), 'YYYY-MM-DD') AS month,\n",
      "    round(avg(price) FILTER (WHERE ${condition})) AS filter_price,\n",
      "    round(avg(price)) AS avg\n",
      "FROM uk_price_paid\n",
      "GROUP BY month\n",
      "ORDER BY month ASC;\n",
      "```\n",
      "\n",
      "#### Query 4: `stats`\n",
      "```sql\n",
      "SELECT\n",
      "    avg(price) AS avg,\n",
      "    percentile_cont(0.5) WITHIN GROUP (ORDER BY price) AS median,\n",
      "    percentile_cont(0.95) WITHIN GROUP (ORDER BY price) AS \"95th\",\n",
      "    percentile_cont(0.99) WITHIN GROUP (ORDER BY price) AS \"99th\",\n",
      "    count(*) AS sold\n",
      "FROM uk_price_paid\n",
      "WHERE date > current_date - INTERVAL '6 months' AND ${condition};\n",
      "```\n",
      "\n",
      "#### Query 5: `priceIncrease`\n",
      "```sql\n",
      "SELECT\n",
      "    round(avg(price) FILTER (WHERE ${condition})) AS filter_avg,\n",
      "    round(avg(price)) AS avg,\n",
      "    EXTRACT(YEAR FROM date) AS year\n",
      "FROM uk_price_paid\n",
      "GROUP BY year\n",
      "ORDER BY year ASC;\n",
      "```\n",
      "\n",
      "#### Query 6: `getRanks`\n",
      "```sql\n",
      "SELECT \n",
      "    percentile_cont(ARRAY[${quantiles}]) WITHIN GROUP (ORDER BY price) AS quantiles,\n",
      "    round(avg(price) FILTER (WHERE ${condition})) AS filtered_avg,\n",
      "    round(avg(price)) AS avg\n",
      "FROM uk_price_paid\n",
      "WHERE date > current_date - INTERVAL '6 months';\n",
      "```\n",
      "\n",
      "#### Query 7: `numberByDuration`\n",
      "```sql\n",
      "SELECT\n",
      "    duration as name,\n",
      "    count(*) FILTER (WHERE ${condition})::int AS value\n",
      "FROM uk_price_paid\n",
      "WHERE duration = 'freehold' OR duration = 'leasehold'\n",
      "GROUP BY duration;\n",
      "```\n",
      "\n",
      "#### Query 8: `numberByType`\n",
      "```sql\n",
      "SELECT\n",
      "    type,\n",
      "    count(*) FILTER (WHERE ${condition})::int AS filtered_count,\n",
      "    round(count(*)::numeric / COUNT(DISTINCT ${column})) AS count\n",
      "FROM uk_price_paid\n",
      "GROUP BY type;\n",
      "```\n",
      "\n",
      "#### Query 9: `soldByPeriod`\n",
      "```sql\n",
      "SELECT \n",
      "    count(*) FILTER (WHERE date >= current_date - INTERVAL '6 months') AS \"6\",\n",
      "    count(*) FILTER (WHERE date >= current_date - INTERVAL '12 months' AND date < current_date - INTERVAL '6 months') AS \"12\",\n",
      "    count(*) FILTER (WHERE date >= current_date - INTERVAL '18 months' AND date < current_date - INTERVAL '12 months') AS \"18\",\n",
      "    count(*) FILTER (WHERE date >= current_date - INTERVAL '24 months' AND date < current_date - INTERVAL '18 months') AS \"24\"\n",
      "FROM uk_price_paid\n",
      "WHERE ${condition};\n",
      "```\n",
      "\n",
      "#### Query 10: `priceByType`\n",
      "```sql\n",
      "SELECT\n",
      "    type,\n",
      "    round(min(price)) + 100 AS min,\n",
      "    round(min(price) FILTER (WHERE ${condition})) AS min_filtered,\n",
      "    round(max(price)) AS max,\n",
      "    round(max(price) FILTER (WHERE ${condition})) AS max_filtered,\n",
      "    round(percentile_cont(0.5) WITHIN GROUP (ORDER BY price)) AS median,\n",
      "    round(percentile_cont(0.5) WITHIN GROUP (ORDER BY price) FILTER (WHERE ${condition})) AS median_filtered,\n",
      "    round(percentile_cont(0.25) WITHIN GROUP (ORDER BY price)) AS \"25th\",\n",
      "    round(percentile_cont(0.25) WITHIN GROUP (ORDER BY price) FILTER (WHERE ${condition})) AS \"25th_filtered\",\n",
      "    round(percentile_cont(0.75) WITHIN GROUP (ORDER BY price)) AS \"75th\",\n",
      "    round(percentile_cont(0.75) WITHIN GROUP (ORDER BY price) FILTER (WHERE ${condition})) AS \"75th_filtered\"\n",
      "FROM uk_price_paid\n",
      "GROUP BY type;\n",
      "```\n",
      "\n",
      "#### Query 11: `salesByDayPreviousYear`\n",
      "```sql\n",
      "SELECT\n",
      "    EXTRACT(YEAR FROM date) AS year,\n",
      "    date_trunc('day', date) AS day,\n",
      "    count(*) AS c\n",
      "FROM uk_price_paid\n",
      "WHERE ${condition}\n",
      "AND date >= date_trunc('year', current_date) - INTERVAL '1 year'\n",
      "AND date < date_trunc('year', current_date)\n",
      "GROUP BY year, day\n",
      "ORDER BY year ASC, day ASC;\n",
      "```\n",
      "\n",
      "#### Query 12: `salesByDayCurrentYear`\n",
      "```sql\n",
      "SELECT\n",
      "    EXTRACT(YEAR FROM date) AS year,\n",
      "    date_trunc('day', date) AS day,\n",
      "    count(*) AS c\n",
      "FROM uk_price_paid\n",
      "WHERE ${condition}\n",
      "AND date >= date_trunc('year', current_date)\n",
      "GROUP BY year, day\n",
      "ORDER BY year ASC, day ASC;\n",
      "```\n",
      "\n",
      "#### Query 13: `soldByDuration`\n",
      "```sql\n",
      "SELECT\n",
      "    duration AS name,\n",
      "    count(*) AS value\n",
      "FROM uk_price_paid\n",
      "WHERE duration != 'unknown' AND ${condition}\n",
      "GROUP BY duration;\n",
      "```\n",
      "\n",
      "#### Query 14: `soldByDurationNoFilter`\n",
      "```sql\n",
      "SELECT\n",
      "    duration AS name,\n",
      "    count(*) AS value\n",
      "FROM uk_price_paid\n",
      "WHERE duration != 'unknown'\n",
      "GROUP BY duration;\n",
      "```\n",
      "\n",
      "#### Query 15: `getMinMax`\n",
      "```sql\n",
      "SELECT\n",
      "    round(percentile_cont(0.01) WITHIN GROUP (ORDER BY price)) AS min_price,\n",
      "    round(percentile_cont(0.99) WITHIN GROUP (ORDER BY price)) AS max_price\n",
      "FROM uk_price_paid\n",
      "WHERE date > (current_date - interval '18 months') AND ${condition}\n",
      "```\n",
      "\n",
      "#### Query 16: `getPopularTowns`\n",
      "```sql\n",
      "SELECT \n",
      "    town, count(*) as popularity \n",
      "FROM uk_price_paid \n",
      "WHERE district = '${district}' \n",
      "GROUP BY district, town \n",
      "ORDER BY popularity DESC \n",
      "LIMIT 10\n",
      "```\n",
      "\n",
      "#### Query 17: `getPopularDistricts`\n",
      "```sql\n",
      "SELECT \n",
      "    district, count(*) as popularity \n",
      "FROM uk_price_paid \n",
      "GROUP BY district \n",
      "ORDER BY popularity DESC \n",
      "LIMIT 10\n",
      "```\n",
      "\n",
      "#### Query 18: `getPopularPostcodes`\n",
      "```sql\n",
      "SELECT \n",
      "    postcode1, count(*) as popularity \n",
      "FROM uk_price_paid \n",
      "WHERE town = '${town}' AND district = '${district}' AND postcode1 != '' \n",
      "GROUP BY district, town, postcode1 \n",
      "ORDER BY popularity DESC \n",
      "LIMIT 10\n",
      "```\n",
      "\n",
      "#### Query 19: `getHouseSales`\n",
      "```sql\n",
      "SELECT count(*) FILTER (WHERE ${condition})::int AS area_count, count(*) as national_count FROM uk_price_paid\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Note**: The repository also contains several embedded INSERT statements in Python scripts for data import purposes, but these are not analytics queries. All schema creation queries (CREATE TABLE statements) were excluded as they are not analytics queries per the filtering criteria.\n",
      "\n",
      "\n",
      "Great! The code_reader has found 17 PostgreSQL analytics queries in the repository, all contained in the file `/app/lib/analytics_queries.ts`. These queries perform various analytics on UK house price data including:\n",
      "\n",
      "1. Time-based aggregations (sales over time, prices over time)\n",
      "2. Statistical calculations (median, percentiles, averages)\n",
      "3. Property type and duration analysis\n",
      "4. Geographic popularity rankings\n",
      "5. Price distribution analysis\n",
      "\n",
      "Now I need to use the code_converter to convert these PostgreSQL queries to ClickHouse format. I'll pass all the query information to the converter.Excellent! I found 17 PostgreSQL analytics queries in your repository. Now I'll convert these queries from PostgreSQL to ClickHouse format.\n",
      "Tool #2: code_converter\n",
      "The code_converter has analyzed the PostgreSQL queries and provided a comprehensive overview of what they do, but I notice it didn't actually provide the converted ClickHouse queries. I need to ask the converter to actually convert the queries to ClickHouse format. Let me try again with a more specific request.I see the converter analyzed the queries but didn't provide the actual ClickHouse conversions. Let me request the specific conversions:\n",
      "Tool #3: code_converter\n",
      "Perfect! The code_converter has successfully converted all the PostgreSQL queries to ClickHouse format. Now I need to use the code_writer to replace the original PostgreSQL queries in the repository with these new ClickHouse queries. I'll pass the converted code to the code_writer.Perfect! Now I have all the converted ClickHouse queries. Let me write these back to the repository to replace the original PostgreSQL queries.\n",
      "Tool #4: code_writer\n"
     ]
    }
   ],
   "source": [
    "# Run the workflow with a test case\n",
    "repo_path = \"/Users/eldimi/Documents/ch/HouseClick\"\n",
    "\n",
    "print(f\"üìã Repository path: {repo_path}\\n\")\n",
    "\n",
    "try:\n",
    "    result = orchestrator.run_workflow(repo_path)\n",
    "    \n",
    "    print(f\"\\nüìä Workflow Result:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Workflow execution failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try without Interleaved Thinking\n",
    "\n",
    "Experiment with calling the orchestrator and disabling interleaved thinking. Observe the difference in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try the same task WITHOUT interleaved thinking\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ Now running the same task WITHOUT interleaved thinking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    result_without_thinking = orchestrator.run_workflow(test_case, enable_interleaved_thinking=False)\n",
    "    \n",
    "    print(f\"\\nüìä Workflow Result (Without Interleaved Thinking):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result_without_thinking)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Workflow execution failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
