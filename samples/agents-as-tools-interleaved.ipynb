{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents as Tools with Strands Agents SDK and Claude 4 Interleaved Thinking\n",
    "\n",
    "This notebook demonstrates how to use Strands Agents SDK with Claude 4's **interleaved thinking** capability to orchestrate intelligent workflows with specialist agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Interleaved Thinking\n",
    "\n",
    "### What is Interleaved Thinking?\n",
    "\n",
    "Interleaved thinking is a new capability in Claude 4 models that allows the model to:\n",
    "\n",
    "1. **Think between tool calls**: Process and reason about results before deciding next steps\n",
    "2. **Chain multiple tools with reasoning**: Make sophisticated multi-step decisions\n",
    "3. **Adapt strategies dynamically**: Change approach based on intermediate results\n",
    "\n",
    "### How It Works\n",
    "\n",
    "There are a lot of similarities between Agent's event loop implemented with and without interleaved thinking:\n",
    "```\n",
    "Query ‚Üí LLM is thinking -> LLM decides to call a Tool -> Event Loop calls the Tool -> Ouput is sent back to LLM -> [ this continues until LLM no longer needs to call any tools - it rendered the Final Answer ]\n",
    "```\n",
    "\n",
    "The main difference you'll notice with the interleaved thinking is that Event loop is acting on LLM's \"thoughts\", rather than \"decisions\". Notice the second link in the loop above, called \"thinking\". In a traditional event loop, the thoughts are hidden. We have to wait until LLM renders either a decision to call a tool or produces the Final Answer. \n",
    "\n",
    "In case of interleaved thinking, LLM is \"leaking\" its thoughts into the even loop while it's still in that second step - \"LLM is thinking\" - and event loop is configured to executed the tools as soon as LLM \"thinks\" about doing it. What this means is that by the time LLM is done thinking, it actually has the Final Answer, on the very first \"decision\". \n",
    "\n",
    "\n",
    "### Enabling Interleaved Thinking\n",
    "\n",
    "To enable this feature with Strands and Bedrock:\n",
    "- Set `temperature=1` (required when thinking is enabled)\n",
    "- Add beta header: `\"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"]`\n",
    "- Configure reasoning budget: `\"reasoning_config\": {\"type\": \"enabled\", \"budget_tokens\": 3000}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install strands-agents strands-agents-tools python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.models import bedrock\n",
    "\n",
    "# bedrock.DEFAULT_BEDROCK_MODEL_ID = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "bedrock.DEFAULT_BEDROCK_MODEL_ID = \"us.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Specialist Agents as Tools\n",
    "\n",
    "First, we are going to create four specialist agents using the Strands `@tool` decorator:\n",
    "- **Researcher**: Gathers factual information\n",
    "- **Data Analyst**: Processes and analyzes information\n",
    "- **Fact Checker**: Verifies information accuracy\n",
    "- **Report Writer**: Creates polished final documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import CODE_ANALYSIS_PROMPT\n",
    "from strands.tools.mcp import MCPClient\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands_tools import file_read\n",
    "\n",
    "#  Specialist agents implemented as tools using Strands @tool decorator\n",
    "@tool\n",
    "def code_reader(repo_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Code reader specialist that can search through a repository and find relevant content.\n",
    "    \n",
    "    Args:\n",
    "        repo_path: The repository path of the repository to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Reading findings\n",
    "    \"\"\"\n",
    "\n",
    "    bedrock_model = BedrockModel(model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\")\n",
    "\n",
    "    result = str()\n",
    "\n",
    "    try:\n",
    "        env = {\"FASTMCP_LOG_LEVEL\": \"DEBUG\",\n",
    "        \"AWS_PROFILE\": \"eldimi-Admin\",\n",
    "        \"AWS_REGION\": \"us-east-1\",\n",
    "                    }\n",
    "\n",
    "        git_repo_mcp_server = MCPClient(\n",
    "            lambda: stdio_client(\n",
    "                StdioServerParameters(\n",
    "                    command=\"uvx\",\n",
    "                    args=[\"awslabs.git-repo-research-mcp-server@latest\"],\n",
    "                    env=env,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with git_repo_mcp_server:\n",
    "\n",
    "            tools = [git_repo_mcp_server.list_tools_sync()]\n",
    "            code_reader_agent = Agent(\n",
    "                model=bedrock_model,\n",
    "                system_prompt=CODE_ANALYSIS_PROMPT,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            result = str(code_reader_agent(repo_path))\n",
    "            print(\"\\n\\n\")\n",
    "    except Exception as e:\n",
    "        return f\"Error processing your query: {str(e)}\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def code_converter(data: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts PostreSQL analytics queries to ClickHouse analytics queries.\n",
    "    \n",
    "    Args:\n",
    "        data: the file paths, code and code description\n",
    "        \n",
    "    Returns:\n",
    "        The converted queries\n",
    "    \"\"\"\n",
    "    # Analyst agent focuses on extracting insights\n",
    "    code_converter_agent = Agent(\n",
    "        model=\"\",\n",
    "        system_prompt=\"\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # Analyze the provided data\n",
    "    result = code_converter_agent(data)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def code_writer(repo_path: str, coverted_code:str) -> str:\n",
    "    \"\"\"\n",
    "    Writes new code in the repository given the provided coverted_code queries\n",
    "    \n",
    "    Args:\n",
    "        repo_path: The path of the repository to write the code to\n",
    "        coverted_code: the converted queries\n",
    "        \n",
    "    Returns:\n",
    "        Fact-check results with accuracy assessment\n",
    "    \"\"\"\n",
    "    code_writer_agent = Agent(\n",
    "        model=\"\",\n",
    "        system_prompt=\"\",\n",
    "        callback_handler=None\n",
    "    )\n",
    "    \n",
    "    # Verify the information\n",
    "    result = code_writer_agent(f\"Fact-check this information: {information}\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 4 Orchestrator with Interleaved Thinking\n",
    "\n",
    "Now we create the orchestrator - a Claude 4 agent that uses interleaved thinking to intelligently coordinate the specialist agents.\n",
    "\n",
    "### How the Orchestrator Works:\n",
    "\n",
    "1. Receives a high-level task from the user\n",
    "2. **Thinks** about what information is needed\n",
    "3. Calls the researcher tool to gather initial data\n",
    "4. **Thinks** about the research results and what analysis is needed\n",
    "5. Calls the data analyst to process findings\n",
    "6. **Thinks** about accuracy and verification needs\n",
    "7. May call the fact checker if needed\n",
    "8. **Thinks** about how to present the findings\n",
    "9. Calls the report writer for final output\n",
    "10. **Reflects** on the complete workflow before responding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4 Orchestrator with Interleaved Thinking using Strands\n",
    "class StrandsInterlevedWorkflowOrchestrator:\n",
    "    def __init__(self):\n",
    "        # Define the orchestrator system prompt for intelligent workflow coordination\n",
    "        self.system_prompt = \"\"\"You are an intelligent workflow orchestrator with access to specialist agents.\n",
    "\n",
    "        Your role is to intelligently coordinate a workflow using these specialist agents:\n",
    "        - code_reader: Reads a repository content from the file system and searches for ALL postgreSQL analytics queries\n",
    "        - code_converter: Converts the found postgres analytics queries to ClickHouse analytics queries\n",
    "        - code_writer: Replaces the postgres analytics queries implementation with the new ClickHouse analytics queries  \n",
    "\n",
    "        The agents will run sequentially code_reader -> code_converter -> code_writer. \n",
    "        The code_reader might need to run multiple times to identify ALL postgreSQL queries.\n",
    "        The coordinator can re-try and validate accordingly.\n",
    "        The coordinator understands the output of each agent and provides the output if needed as input to the next agent.\n",
    "        \"\"\"\n",
    "    \n",
    "    def run_workflow(self, task: str, enable_interleaved_thinking: bool = True) -> str:\n",
    "        \"\"\"Execute an intelligent workflow for the given task.\n",
    "        \n",
    "        Args:\n",
    "            task: The task to complete\n",
    "            enable_interleaved_thinking: Whether to enable interleaved thinking (default: True)\n",
    "        \n",
    "        The orchestrator will:\n",
    "        1. Understand the task requirements\n",
    "        2. Think about the best approach\n",
    "        3. Coordinate specialist agents\n",
    "        4. Reflect on results between steps\n",
    "        5. Produce a comprehensive output\n",
    "        \"\"\"\n",
    "        thinking_mode = \"WITH interleaved thinking\" if enable_interleaved_thinking else \"WITHOUT interleaved thinking\"\n",
    "        print(f\"\\nStarting intelligent workflow {thinking_mode} for: {task}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Configure Claude 4 with or without interleaved thinking via Bedrock\n",
    "        if enable_interleaved_thinking:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1,  # Required to be 1 when thinking is enabled\n",
    "                additional_request_fields={\n",
    "                    # Enable interleaved thinking beta feature\n",
    "                    \"anthropic_beta\": [\"interleaved-thinking-2025-05-14\"],\n",
    "                    # Configure reasoning parameters\n",
    "                    \"reasoning_config\": {\n",
    "                        \"type\": \"enabled\",  # Turn on thinking\n",
    "                        \"budget_tokens\": 3000  # Thinking token budget\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            claude4_model = BedrockModel(\n",
    "                model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "                max_tokens=4096,\n",
    "                temperature=1\n",
    "            )\n",
    "        \n",
    "        # Create the orchestrator agent with Claude 4 and specialist tools\n",
    "        orchestrator = Agent(\n",
    "            model=claude4_model,\n",
    "            system_prompt=self.system_prompt,\n",
    "            tools=[code_reader, code_converter, code_writer]\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"Complete this task using intelligent workflow coordination: {task}\n",
    "\n",
    "        Instructions:\n",
    "        1. Think carefully about what information you need to accomplish this task\n",
    "        2. Use the specialist agents strategically - each has unique strengths\n",
    "        3. After each tool use, reflect on the results and adapt your approach\n",
    "        4. Coordinate multiple agents as needed for comprehensive results\n",
    "        5. Ensure accuracy by fact-checking when appropriate\n",
    "        6. Provide a comprehensive final response that addresses all aspects\n",
    "        \n",
    "        Remember: Your thinking between tool calls helps you make better decisions.\n",
    "        Use it to plan, evaluate results, and adjust your strategy.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = orchestrator(prompt)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Workflow failed: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Demo\n",
    "\n",
    "Let's see the orchestrator in action! Watch how it thinking and making tool calling while it's thinking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the orchestrator\n",
    "print(\"Strands Agents SDK: Claude 4 Interleaved Thinking Workflow Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    orchestrator = StrandsInterlevedWorkflowOrchestrator()\n",
    "    print(\"‚úÖ Orchestrator initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize orchestrator: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow with a test case\n",
    "repo_path = \"/Users/eldimi/Documents/ch/updt/HouseClick\"\n",
    "\n",
    "print(f\"üìã Repository path: {repo_path}\\n\")\n",
    "\n",
    "try:\n",
    "    result = orchestrator.run_workflow(repo_path)\n",
    "    \n",
    "    print(f\"\\nüìä Workflow Result:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Workflow execution failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try without Interleaved Thinking\n",
    "\n",
    "Experiment with calling the orchestrator and disabling interleaved thinking. Observe the difference in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try the same task WITHOUT interleaved thinking\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîÑ Now running the same task WITHOUT interleaved thinking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    result_without_thinking = orchestrator.run_workflow(test_case, enable_interleaved_thinking=False)\n",
    "    \n",
    "    print(f\"\\nüìä Workflow Result (Without Interleaved Thinking):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result_without_thinking)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Workflow execution failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
